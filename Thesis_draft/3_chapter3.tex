The task we are trying to solve is quite peculiar: we are trying to impute the missing values in the data, not to perform a statistical analysis, but to select and train a prediction model. Our final benchmark of performance is not parameter estimation but predictive loss. As the next two chapters will show, although this seems like a small difference, this actually leads to some major changes.

It is interesting to note in that regard how the communities of statistics and machine learning seem to lack any point of convergence on the subject. Missing data have been an active field of research in statistics for a long time \cite{rubin1976inference}, and many complex methods have been developed and proven for statistical inference \cite{Rubin_missdata} (such as those described in Chapter \ref{imputation}).

On the other hand, there is almost no research on these methods when applied to prediction. Even recent manuals for machine learning \cite{ML_missdata} generally make only a quick mention of missing data, and in practice it is extremely rare for anything else than imputation by the mean to be used. \emph{Scikit-learn} \cite{scikit-learn}, by far the most-used machine learning package, only proposes imputation by the mean as of now. 

A few research papers \cite{prediction_imputation1} \cite{prediction_imputation2} try to assess the performance of more recent imputation methods when used in a predictive context. However, they do not propose any framework or theory on this endeavour. They take it for granted that they can impute the whole dataset before performing the subsequent analysis. However, when performing prediction there is one significant difference with statistical inference, which we describe further in this chapter: the data is split into one dataset to learn the model, and another one to validate its performance. This raises many questions that we discuss here and in Chapter \ref{linreg}. 

After laying out the general framework of Empirical Risk Minimization \cite{ERM}, which is the general paradigm used for prediction, we adapt it to fit the context of missing data. We notice that current implementations of modern imputation methods are incompatible with this framework, and try to devise solutions for this issue.
	\section{Empirical risk minimization (ERM): classical context}
We first describe Empirical Risk Minimization (ERM) without missing data, as described in \cite{ERM}.
		\subsection{Context and notations}
We are provided with a matrix $X$ of size $n \times p$ and response vector $y$ of size $n$. Our goal is to learn a model in the form $\hat{y} = f(x, \beta)$, where $\beta$ is some parameter to choose, $\hat{y}$ is a predicted value for $y$ and $f$ is a fixed parametric predictive function (usually corresponding to a choice of function in a given class).

The quality of a prediction is evaluated with the loss $L(y,\hat{y})$. The end goal in this context is to find the parameter $\hat{\beta}$ which minimizes the risk: $R = \mathbb{E}(L(y, f(X)))$. However, we do not have access to the real expectation of the risk, so we must use a proxy for this value. We define the empirical risk:

$$ R_{\text{emp}}(y, f(X, \beta)) = \frac{1}{n} \sum\limits_{i=1}^n L(y_i, f(X_i, \beta))$$

Empirical Risk Minimization corresponds to choosing $\beta$ minimizing $R_{emp}$ for our $X$ and $y$. However, this is not enough. We do not just want the optimal $\beta$, we also want a measure of how well the final model would perform on new data. This is important because this is what we will take into account to make our choice of $f$. But the empirical risk gives us no measure of how well our model generalizes whatsoever, only of how closely it can fit known data.

To address the issue of model selection, the standard practice is to measure the error on some data that were not used to learn the model: it is the principle of cross-validation.

		\subsection{Cross-validation}
To perform Cross-validation, we divide the data in two datasets: first we choose $n_A < n$ entries in the dataset to be used to loearn $\beta$: this is the training dataset $X_A$ and response $y_A$. We denote $I_A = (i_1, \ldots, i_{n_A})$ the set of indices chosen for the training data. The rest of the observations are noted $X_V$ and $y_V$ and called the validation dataset.

Once this is done, ERM is performed as before, using only the training data. The obtained parameter $\hat{\beta}$ can then be evaluated with the validation error:

$$ R_{V} = \frac{1}{n_V} \sum\limits_{i=1, i \notin I_A}^n L(y_i, f(X_i, \hat{\beta}))$$

It is this value that we can compare to choose our our model class $f$.

	\section{ERM with missing data: the problem of current methodologies}
We now place ourselves in the same context as before, except some values are missing from $X$, both in the training and the validation data. The context is almost the same as before: choosing a parametric model that takes as input the observed data and outputs a prediction for $y$.
		\subsection{Imputation seen as an ERM}
Remember that the purpose of this work is to impute the data independently of the predictive model used afterwards. This does not change the framework of ERM but it does mean that we cannot use any function we like to go from $X$ to $\hat{y}$. The prediction is the composition of two steps.
			\paragraph{Imputation step}
First we choose an imputation model $X^{\text{complete}} = g(X, \alpha)$ where $X^{\text{complete}}$ is the completed dataset and $\alpha$ some parameter. This is similar to predicting $y$ as we did previously with one caveat: we do not know the true data, even on the training dataset. Thus we choose $\hat{\alpha}$ to minimize some unsupervised loss
$$L'(g(X, \alpha), \alpha)$$
 measuring the fit of the model to the data. Generally, this means that we choose a parameter that maximizes the likelihood of the observed data according to some generative model (though it is not always the case). Once this is done, we obtain a completed dataset $\hat{X}$.

			\paragraph{Prediction step}
Once the imputation is done, we can perform as before to choose a parameter $\hat{\beta}$ that minimizes the empirical risk when using the completed data:
	$$ R_{\text{emp}}(y, f(\hat{X}_i, \beta)) = \frac{1}{n} \sum\limits_{i=1}^n L(y_i, f(\hat{X}_i, \beta))$$
	
Putting it all together, we can define 
$$ h(X, (\beta, \alpha)) = f(X^{imp}, \beta) = f( g(X, \alpha), \beta) $$

the combined model that takes the observed data as input and outputs a predicted $y$. This is optimized as 
\begin{align*}
\hat{\alpha} &= \argmin_{\alpha} L'(g(X,\alpha, \alpha) \\
\hat{\beta} &= \argmin_{\beta} R_{\text{emp}}(y, h(X, (\beta, \hat{\alpha})))
\end{align*}

We choose to use this notation to illustrate our point that imputation is an integral part of the ERM, not a separate, preliminary process. In particular, it means that in theory its parameters \emph{must} be subjected to cross-validation just like those of the prediction. That is $X_A$ the training data are used to estimate $(\hat{\beta}, \hat{\alpha})$ as shown just above. Then, we can compute a prediction $\hat{y}_V = h(X_V, (\hat{\beta}, \hat{\alpha}))$ which can be compared to $y_V$ to evaluate the choice of model.

The bottom line is that just like for the prediction, the imputation parameter $\alpha$ should be estimated only on the training data and then used on the validation data. As we will see, this raises an issue with the way current imputation methods are implemented.

		\subsection{Unsuitability of current methods}
Over the years, many imputation methods have been proposed, and we describe some in Chapter \ref{imputation}. They have various assumptions and principles but they have one thing in common: they are all implemented (usually in R) via a single function. That function takes a dataset with missing values as an input and returns the dataset completed by the method of choice, \emph{without giving the user any access to the model itself}.

This is a problem because of how cross-validation is supposed to be performed. Normally, one would estimate $(\hat{\beta}_A, \hat{\alpha}_A)$ through ERM, and then make a prediction on the validation set as $h(X_V,(\hat{\beta}_{X_A},\hat{\alpha}_{X_A}))$. But here, all we have access to is a function $g': X \mapsto g(X, \hat{\alpha}_X)$ where $\hat{\alpha}_X$ is the optimised parameter for the argument $X$. It is straightforward to see that using such a function, one can no longer separate the estimation of the parameters and the imputation in itself.
	
Incidentally, this issue was mentioned recently \cite{github_sklearn}, as part of a larger effort to port the MICE \cite{MICE_founding} imputation method to Scikit-learn \cite{scikit-learn}. This package is developed for data scientists, who are used to having separate functions for training and prediction. Likewise, several other mentions of this problem arose in the last two years \cite{thread_newdata1}\cite{thread_newdata2}\cite{thread_newdata3} with no mention of it earlier. It seems that as more elaborate imputation methods become popular with the machine learning community, the need arises for a change in the way those methods are implemented. However, it is clear that such changes take time, and for now we can do one of two things: make do with what is available, or build our own implementation to adapt one of these methods to our goals.
	\section{Possible solutions}
		\subsection{Using current implementations}
			\subsubsection{Methods}
There are many ways we can use to approximate the correct procedure, though none of them is theoretically satisfactory. We describe the most natural ones and their caveats.

\paragraph{Grouped imputation} Impute all of the data at once before performing the cross-validation split. That is, estimate $\hat{\alpha}_X$ on the whole data then $$\hat{\beta}_{X_A} = \argmin_{\beta} R_{emp}(y, h(X, (\beta, \hat{\alpha}_X)))$$
 and $\hat{y}_V = h(X_V, (\beta_{X_A}, \alpha_X))$. 

In that case, the parameter $\alpha$ is imputed using the validation data. In particular, imputed values in $X_A$ \emph{depend on those in $X_V$}, which is contrary to the basic principles of cross-validation where validation data must be held out during parameter estimation. In theory this could falsify the validation error by making it too optimistic (if some hard-to-predict observations are present in the test set, that would otherwise have high validation error but will not in this case because of our 'cheating').

\paragraph{Separate imputation}
Divide the data first, then impute each dataset separately:
\begin{align*}
\hat{\alpha}_{X_A} &= \argmin_{\alpha} R_{emp}'(X_A, \alpha) \\
\hat{\alpha}_{X_V} &= \argmin_{\alpha} R_{emp}'(X_V, \alpha) \\
\hat{\beta}_{X_A} &= \argmin_{\beta} R_{emp}(y, h(X, (\beta, \hat{\alpha}_{X_A}))) \\
\hat{y}_V &= h(X_V, (\beta_{X_A}, \alpha_{X_V}))
\end{align*}

Contrarily to grouped imputation, this does not violate cross-validation at all. However, we are using parameter $\alpha_{X_V}$ to impute the validation data and then apply a predictive model that uses $\beta_{X_A}$, optimized for data imputed with $\alpha_{X_A}$. If sample size is not large enough, it is possible that $\alpha_{X_A}$ and $\alpha_{X_V}$ will be noticeably different. In that case, there is no guarantee that $\beta_{X_A}$ will still be valid for prediction on the validation dataset that was imputed differently. 

\paragraph{Line by line imputation}
The last option we mention here is to first impute the training data on its own. Then for every line of the validation data we impute it by stacking it with the imputed training data and imputing the whole dataset. Since it is just one line, we can safely assume that the imputation parameters will be those of the training data. The main issue with this method is that is the validation data is rather large, this will be computationally infeasible.

			\subsubsection{Need for a correct implementation}
We want to understand if the alternatives proposed here are good enough to be used if a correct implementation is impossible. To do that, we need to be able to compare these with the correct method. That means that for at least one imputation method we need to build an implementation that allows us to separate the estimation and the imputation. That way we will be able to compare its performance with the other alternatives we propose

Moreover, in addition to this theoretical pursuit, we need this because of what we are trying to achieve with Traumabase: the end goal is to make a recommendation system that can produce a prediction for \emph{a single new patient} arriving to the hospital, ideally without needing to have access to the whole Traumabase data (which is hard to share because it contains sensitive patient information). Without access to the initial training data, this means that only a fully parametric approach can be taken in this particular case (separate imputation is impossible on just one line of data, and the other ones require access to the full data).

Below, we design a very simple imputation method for those purposes.

		\subsection{A new variant: Multivariate Normal conditional expectation with reserved data}
The principle of this imputation is inspired from R package \emph{Amelia} \cite{ref_amelia}, and a large part of the code is from the \emph{norm} package \cite{pkg_norm}. The idea is to assume that both $X_A$ and $X_V$ follow a normal distribution $\mathcal{N}(\mu, \Sigma)$ with unknown parameters.

\paragraph{Parameter estimation}
It is possible to approximate maximum-likelihood estimators for $\mu$ and $\sigma$ iteratively, using the EM algorithm \cite{EM} \cite{em_normal_fit}. First, the missing values of $X$ are imputed randomly to give $X^{(1)}$. Then we repeat the following steps:

For $t=1...$
\begin{enumerate}
\item $(\mu^{(t)}, \Sigma^{(t)})$ are computed as the maximum likelihood estimators on the completed data $X^{(t)}$ for the normal distribution (empirical mean and covariance)
\item $X^{(t+1)}$ is computed by replacing the missing values by their expected value under the new parameters: 
$$X^{(t+1)} = \mathbb{E}(X \vert X^{\text{obs}} ; \mu^{(t)}, \Sigma^{(t)})$$

The conditional expectation is easily derived using the Schur complement \cite{norm_schur}.
\end{enumerate}

Once this converges we have an estimated parameter $\hat{\alpha} = (\hat{\mu}, \hat{\Sigma})$.

\paragraph{Imputation}
Once we have the parameters, it is very straightforward to get an imputation of the missing data. There are two different ways of doing this:
\begin{itemize}
\item Replace missing values by their expectation conditional on the observed data: 
$$\hat{X} = \mathbb{E}(X \vert X^{\text{obs}} ; \hat{\mu}, \hat{\Sigma})$$
That is, pick the mean of the conditional distribution.
\item Draw the missing values from the conditional distribution
\end{itemize}

In the second case, the \emph{norm} package provides all that is needed. This is what would be used for multiple imputation. However, as we see in Chapter \ref{linreg}, the mean is the optimal choice if we want a single best imputation to use for a prediction model. We implemented this method. We use it below in order to compare the methods mentioned before.

		\subsection{Comparison on simulated data}
In order to compare the methods used for imputation
		
\addtocontents{toc}{\protect\vspace*{\fill}}
\addtocontents{toc}{\protect\newpage} % used to split the toc over two pages		
\addtocontents{toc}{\protect\vspace*{\fill}}
