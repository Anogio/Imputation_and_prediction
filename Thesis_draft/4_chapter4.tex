In order to make good decisions for imputation, it is important to understand how it impacts prediction. To gain a better understanding of this issue, we solve a very simple case of cross-validated regression with missing data. Although quite restrictive, this situation provides some insights into the way that missing data impacts prediction performance.

	\section{Problem set-up}
We place ourselves in a linear regression setup with cross-validation (cf Chapter \ref{validation}). The data is split between a training dataset $X_A, y_A$ and validation dataset $X_V, y_V$. We use the multi-agent framework described in \ref{framework}.
		\subsection{Notations}
			\subsubsection{God's data}
The response variable is a noisy linear combination of the covariates in $X$:
\begin{equation*}
\tilde{X}_A = 
\begin{pmatrix}
x_{11} & x_{12} \\
\vdots & \vdots \\
x_{n1} & x_{n2}
\end{pmatrix}
\quad \mathrm{and} \quad
y_A = X_A \beta + \epsilon_A
\quad \mathrm{with} \quad
\epsilon_A \sim \mathcal{N}(0, \sigma^2)
\end{equation*}
\begin{equation*}
\tilde{X_V} = 
\begin{pmatrix}
x_{11}^V & x_{12}^V \\
\vdots & \vdots \\
x_{n_V1}^V & x_{n_V2}^V
\end{pmatrix}
\quad \mathrm{and} \quad
y_V = X_V \beta + \epsilon_V
\quad \mathrm{with} \quad
\epsilon_V \sim \mathcal{N}(0, \sigma^2)
\end{equation*}


			\subsubsection{Observed data}
The observed data is God's data with some missing values. Specifically, some observations are missing from the first column of each dataset.  We observe the full $y^A$, but the covariate matrices we actually have access to are:
\begin{equation*}
X_A = 
\begin{pmatrix}
? & x_{12} \\
\vdots & \vdots \\
? & x_{k_A2} \\
x_{(k_A+1)1} & x_{(k_A+1)2}\\
\vdots & \vdots \\
x_{n1} & x_{n2}
\end{pmatrix}
\end{equation*}

which is sent to the Imputer, and

\begin{equation*}
X_V = 
\begin{pmatrix}
? & x_{12}^V \\
\vdots & \vdots \\
? & x_{k_V 2}^V \\
x_{(k_V+1)1}^V & x_{(k_V+1)2}^V\\
\vdots & \vdots \\
x_{n_V 1}^V & x_{n_V 2}^V
\end{pmatrix}
\end{equation*}

which is sent to the Practitioner. That is, there are $k_A$ and $k_v$ missing values in the datasets (the mechanism is MCAR).

Note that the datasets have a different status. The training dataset is available to the Imputer then the Analyst at the time of analysis, it is some given historical data. The validation dataset is some future data available only to the Practitioner who will perform a black-box prediction based on the Analyst's and the Imputer's indications. That is why when we take expectations in this chapter, we will condition only on the observed data $X_A$ while we integrate on $X_V, \epsilon_A, \epsilon_V$ and the missing data $X_A^{\text{miss}}, X_V^{\text{miss}}$ which are all unknowns at the time of analysis. 

		\subsection{Imputed data and regression}
			\subsubsection{Principle}
The Imputer fits an imputation model $g(\cdot, \alpha)$ and fills in $X_A$ and instructs the Practitioner on how to impute $X_V$. The resulting filled-in datasets are:

\begin{equation*}
\hat{X}_A = 
\begin{pmatrix}
g(x_{12},\hat{\alpha}) & x_{12} \\
\vdots & \vdots \\
g(x_{k_A 2},\hat{\alpha}) & x_{k_A 2} \\
x_{(k_A+1)1} & x_{(k_A+1)2}\\
\vdots & \vdots \\
x_{n 1} & x_{n 2}
\end{pmatrix}
\quad \mathrm{and} \quad
\hat{X}_V = 
\begin{pmatrix}
g(x_{12}^V,\hat{\alpha}) & x_{12}^V \\
\vdots & \vdots \\
g(x_{k_V 2}^V,\hat{\alpha}) & x_{k_V 2}^V \\
x_{(k_V+1)1}^V & x_{(k_V+1)2}^V\\
\vdots & \vdots \\
x_{n_V 1}^V & x_{n_V 2}^V
\end{pmatrix}
\end{equation*}

Then, $\hat{X}_A$ is sent by the Imputer to the Analyst. The Analyst only has access to $\hat{X}_A$ and $y_A$. 
The end goal is to learn an estimator on the training set that minimizes the expected loss on the validation set:
$$
L(y_V, \hat{y}_V) = (y_V - \hat{y_V})^2
$$

In line with the principles of ERM and CV (cf Chapter \ref{validation}), the Analyst minimizes the equivalent quantity in the training set. Assuming a linear relationship between the covariates and response, the least-squares estimate for $\beta$ is standard \cite{seber2012linear}
$$
\hat{\beta}_n = (\hat{X}_A^T \hat{X}_A)^{-1} \hat{X}_A^T y_A 
$$

$\hat{\beta}$ is then transferred to the Practitioner who can use it to compute a prediction
$$\hat{y}_V = \hat{X}_V \hat{\beta}_n $$

which will be compared to $y_V$:

$$L(\hat{y}_V, y_V) = \sum\limits_{i=1}^{n_V} (y_V^{(i)} - \hat{y}_V^{(i)})^2$$
Our end goal is to minimise this metric. 

In what we described above, the actions of the Analyst and the Practitioner are completely determined. On the other hand, we have not specified how the Imputer proceeds to the imputation. We want to investigate the effect of the choice of imputation on the expected loss:

$$R = \mathbb{E}_{\tilde{X_V}, X_A^{miss}, \epsilon_A, \epsilon_V}[(y_V^{(i)} - \hat{y}_V^{(i)})^2 \vert X_A]$$ %\footnote{Even though what we ultimately want is a decision rule for $\phi$ and $\psi$, they are only a function of the observed data $X_A$, $X_V$, which is fixed here. For simplicity of notation, we write $\phi$ and $\psi$ as constant values}

			\subsubsection{Distribution hypotheses}
Lastly, for this last expression to have any meaning, fix the distribution of $\tilde{X}$ the true data.

We assume that $X \sim \pi$ where the lines of $X$ are independent and identically distributed (i.i.d), and that $\pi$ is known to the Imputer. This is unlikely in a real setting, but we explore the best-case scenario where we have all the necessary information to perform the imputation in order to isolate the error terms that are specific to the presence of missing data --- as opposed to bad imputation.

	\section{Analysis}
		\subsection{Expected loss}
To be able to estimate the expected loss, we break it up into several components. We first denote 
$$
\tilde{\beta}_n = (\tilde{X}_A^T \tilde{X}_A)^{-1} \tilde{X}_A^T y_A 
$$

the estimated parameter we would obtain if the training data were completely observed. We consider the loss for the $i^{\text{th}}$ line of validation data $x_i^V$:

\begin{align*}
L_i(y_V, \hat{y}_V) &= (y_V - \hat{y_V})^2 &\\
				   &= (\tilde{x}_i^V \beta + \epsilon_V - \hat{x}_i^V \hat{\beta}_n)^2 &\\
				   &= (\tilde{x}_i^V(\beta - \tilde{\beta}_n) + \tilde{x}_i^V (\tilde{\beta}_n - \hat{\beta}_n) + (\tilde{x}_i^V - \hat{x}_i^V) \hat{\beta}_n + \epsilon_V)^2 & \\
				   &= (\tilde{x}_i^V (\beta - \tilde{\beta}_n))^2 & (1) \\
				   & \quad + (\tilde{x}_i^V (\tilde{\beta}_n-\hat{\beta}_n))^2 &(2) \\
				   & \quad + ((\tilde{x}_i^V - \hat{x}_i^V) \hat{\beta}_n)^2 &(3) \\
				   & \quad +2 \tilde{x}_i^V (\beta - \tilde{\beta}_n) \tilde{x}_i^V (\tilde{\beta}_n - \hat{\beta}_n) & (4) \\
				   & \quad +2 \tilde{x}_i^V (\beta - \tilde{\beta}_n) (\tilde{x}_i^V - \hat{x}_i^V )\hat{\beta}_n & (5) \\
				   & \quad +2 \tilde{x}_i^V (\tilde{\beta}_n - \hat{\beta}_n) (\tilde{x}_i^V - \hat{x}_i^V) \hat{\beta}_n & (6)\\
				   & + \epsilon_V^2 &\\
				   & + \epsilon_V C
\end{align*}

Where $C$ is some term that will not matter (since it will not count in the expectation --- $\epsilon_V$ has zero expectation and is independent of the other terms). 

\paragraph{Influence of missing validation values}

In particular,
\begin{proposition}
\label{prop.linear}
The expected validation error depends linearly with the proportion of missing data in the validation set (with a fixed amount of missing data in the training set).
\end{proposition}
\begin{proof}
The risk we want to minimize is $R_i$ the expectation of this loss, summed over all rows. Since the rows are i.i.d, (so the order does not count), the expected value of terms $(1),(2),(4)$ is the same for all $i$. For terms $(3), (5), (6)$ there are just possibilities: if there is no missing data in the row, these terms are zero. If a value is missing, they are nonzero but their expectation is the same for all lines with missing data. 

Consequently, denoting $r_V = \frac{k_V}{n_V}$ the proportion of missing validation data, we can express the expected loss as:

\begin{align}
\label{eq.loss}
\sum\limits_{i=1}^{n_V} L_i = &\underbrace{\mathbb{E}_{\tilde{X_V}, X_A^{miss}, \epsilon_A, \epsilon_V}[(1) + (2) + (4)\vert X_A]}_{A} + \\ 
r_V& \underbrace{\mathbb{E}_{\tilde{X_V}, X_A^{miss}, \epsilon_A, \epsilon_V}[(3) + (5) + (6)\vert X_A]}_{B} + \sigma^2
\end{align}

Thus, for $X_A$ fixed and for a given imputation rule, the expected loss is $A + \sigma^2 + Br_V$ with $A$ and $B$ fixed, and the expected loss depends linearly on the proportion of missing values. 

\emph{Remark:} It is clear (given that the loss is nonnegative) that $A+B+\sigma^2 \ge 0$, and standard inequalities show that $A$ (of the form $a^2 + b^2 - 2ab$) is nonnegative. On the other hand, it is not clear that $B$ must be nonnegative. However the next proposition shows that in good scenarios, this does not happen.
\end{proof}

\begin{proposition}
\label{prop.increase}
If $\tilde{\beta}$ and $\hat{\beta}$ are consistent, and $\beta_1 \neq 0$, then for $n_A$ large enough the error increases with the proportion of missing validation data.
\end{proposition} 
\begin{proof}
Consistency ensures that $\norm{(\beta-\hat{\beta}}$ and $\norm{(\beta-\tilde{\beta}}$ can be made arbitrarily small by increasing $n$. Using the Cauchy-Schwarz inequality, this implies terms $(5)$ and $(6)$ can be arbitrarily small, and so (using Jensen's inequality) can the norm of their expectaion.

In addition:

\begin{align*}
\mathbb{E}((3)) &=\mathbb{E}[((\tilde{x}_i^V - \hat{x}_i^V) \hat{\beta}_n)^2] \\ 	
			&\ge  \mathbb{E}[((\tilde{x}_i^V - \hat{x}_i^V) \hat{\beta}_n)]^2 \\
			&=  (\mathbb{E}[(x_1^V - \hat{x_1^V})\mathbb{E}[\hat{\beta}_{n1}])^2
\end{align*}

Where $(x_1^V - \hat{x_1^V})$ is nonzero if the observation is missing and $\hat{beta}_{n1}$ can be made arbitrarily close to $\beta_1$ by increasing n (so in particular, nonzero). This means that for $n$ large enough, term $(3)$ dominates $B$ and is positive so $B >0$.
\end{proof}

\paragraph{Consistency}
In linear regression without missing data, the estimation of the parameter is consistent\cite{consistency_linreg}, that is in our case $\tilde{\beta}_n$ is a consistent estimate of $\beta$. Moreover:

\begin{proposition}
If the estimated values are the expected value of the data conditioned on the observed values, $\hat{\beta}_n$ is a consistent estimate for $\beta$.
\end{proposition}
\begin{proof}
%If 
%$f(x_i,\hat{alpha}) = \mathbb{E}[x_i \vert x_{i2}]$ 
%then
(taken from \cite{little1992missingX})
\todo{Include proof from Little \cite{little1992missingX}}
\end{proof}

Note that to do this one must know the exact distribution of $X$.

If we have consistency, additionally to the Proposition \ref{prop.increase}, we see that the only error terms that do not tend to zero are $(3)$ and $\sigma^2$. 

		\subsection{Consequences}
These results highlight the asymmetry between training and validation data. In a very simple case, it give us an intuition of the behavior of the prediction error with missing data.

We see that when the imputation is done well, consistency results ensure that additional observations improve the inference even if they have missing values: in the training data, it is ultimately possible to make up for the unobserved values and have an arbitrarily good parameter estimate.

On the other hand, missingness in the validation set adds some inevitable terms to the error. This is important because although training and validation data have the same distribution by hypothesis, in some cases the missingness could be somehow different between those datasets.  

For instance, in the Traumabase it is possible that missing data comes from doctors who fail to record some values in the database, although the data was available when treating the patient. In that case when real-world prediction are made, there may be less missing values than there were when we performed CV and model selection. On the contrary, maybe some values were collected late in the process and would have been unavailable for the prehospital diagnostic. Propositions \ref{prop.linear} and \ref{prop.increase} show that this matters a lot, and that efforts to reduce the amount of missing data at the time of real-world predictions may have a more direct effect on performance than efforts to reduce missing data in the database (e.g. by improving recording). 

Lastly, presence of term $(3)$ even for large $n$ and consistent estimation also shows that the intrisinc variability of the covariates imposes a limit on the performance of the prediction with missing data. There is a way to decrease its impact, other than improving the collection of data. Its value is linked to the variance of the unobserved values, conditional on the observed ones. By adding more covariates, one can decrease this conditional variance and improve the prediction.

In the next chapter, we take a more empirical approach to investigate similar issues.