In order to make good decisions for imputation, it is important to understand how it impacts prediction. To gain a better understanding of this issue, we solve a very simple case of cross-validated regression with missing data. Although quite restrictive, this situation provides some insights into the way that missing data impacts prediction performance. In particular, we want to understand what makes this situation different from one of pure parameter estimation (i.e., without a Practitioner), and the implications of missing data in a prediction context.

 We show that these objectives are compatible, in the sense that if the imputer performs a good imputation while the analyst uses a good full-data procedure in some sense, then the predictive loss is asymptotically good. In addition, this example makes it clear that the presence of missing values in the validation dataset means that the situation does not boil down to parameter estimation.

	\section{Problem set-up}
We place ourselves in a linear regression setup with cross-validation (cf Chapter \ref{validation}). The data is split between a training dataset $X_A, y_A$ and validation dataset $X_V, y_V$. We use the multi-agent framework described in \ref{framework}.
		\subsection{Notations}
			\subsubsection{God's data}
The response variable is a noisy linear combination of the covariates in $X$:
\begin{equation*}
\tilde{X}_A = 
\begin{pmatrix}
x_{11} & x_{12} \\
\vdots & \vdots \\
x_{n1} & x_{n2}
\end{pmatrix}
\quad \mathrm{and} \quad
y_A = X_A \beta + \epsilon_A
\quad \mathrm{with} \quad
\epsilon_A \sim \mathcal{N}(0, \sigma^2)
\end{equation*}
\begin{equation*}
\tilde{X_V} = 
\begin{pmatrix}
x_{11}^V & x_{12}^V \\
\vdots & \vdots \\
x_{n_V1}^V & x_{n_V2}^V
\end{pmatrix}
\quad \mathrm{and} \quad
y_V = X_V \beta + \epsilon_V
\quad \mathrm{with} \quad
\epsilon_V \sim \mathcal{N}(0, \sigma^2)
\end{equation*}


			\subsubsection{Observed data}
The observed data is God's data with some missing values. Specifically, some observations are missing from the first column of each dataset. We observe the full $y^A$, but the covariate matrices we actually have access to are:
\begin{equation*}
X_A = 
\begin{pmatrix}
? & x_{12} \\
\vdots & \vdots \\
? & x_{k_A2} \\
x_{(k_A+1)1} & x_{(k_A+1)2}\\
\vdots & \vdots \\
x_{n1} & x_{n2}
\end{pmatrix}
\end{equation*}

which is sent to the Imputer, and

\begin{equation*}
X_V = 
\begin{pmatrix}
? & x_{12}^V \\
\vdots & \vdots \\
? & x_{k_V 2}^V \\
x_{(k_V+1)1}^V & x_{(k_V+1)2}^V\\
\vdots & \vdots \\
x_{n_V 1}^V & x_{n_V 2}^V
\end{pmatrix}
\end{equation*}

which is sent to the Practitioner. That is, there are $k_A$ and $k_v$ missing values in the datasets (the mechanism is MCAR).

Note that the datasets have a different status. The training dataset is available to the Imputer then the Analyst at the time of analysis, it is some given historical data. The validation dataset is some future data available only to the Practitioner who will perform a black-box prediction based on the Analyst's and the Imputer's indications. That is why when we take expectations in this chapter, we will condition only on the observed data $X_A$ while we integrate on $X_V, \epsilon_A, \epsilon_V$ and the missing data $X_A^{\text{miss}}, X_V^{\text{miss}}$ which are all unknowns at the time of analysis. 

		\subsection{Imputed data and regression}
			\subsubsection{Principle}
The Imputer fits an imputation model $g(\cdot, \alpha)$ and fills in $X_A$ and instructs the Practitioner on how to impute $X_V$. The resulting filled-in datasets are:

\begin{equation*}
\hat{X}_A = 
\begin{pmatrix}
g(x_{12},\hat{\alpha}) & x_{12} \\
\vdots & \vdots \\
g(x_{k_A 2},\hat{\alpha}) & x_{k_A 2} \\
x_{(k_A+1)1} & x_{(k_A+1)2}\\
\vdots & \vdots \\
x_{n 1} & x_{n 2}
\end{pmatrix}
\quad \mathrm{and} \quad
\hat{X}_V = 
\begin{pmatrix}
g(x_{12}^V,\hat{\alpha}) & x_{12}^V \\
\vdots & \vdots \\
g(x_{k_V 2}^V,\hat{\alpha}) & x_{k_V 2}^V \\
x_{(k_V+1)1}^V & x_{(k_V+1)2}^V\\
\vdots & \vdots \\
x_{n_V 1}^V & x_{n_V 2}^V
\end{pmatrix}
\end{equation*}

Then, $\hat{X}_A$ is sent by the Imputer to the Analyst. The Analyst only has access to $\hat{X}_A$ and $y_A$. 
The end goal is to learn an estimator on the training set that minimizes the expected loss on the validation set:
$$
L(y_V, \hat{y}_V) = (y_V - \hat{y_V})^2
$$

In line with the principles of ERM and CV (cf Chapter \ref{validation}), the Analyst minimizes the equivalent quantity in the training set. Assuming a linear relationship between the covariates and response, the least-squares estimate for $\beta$ is standard \cite{seber2012linear}
$$
\hat{\beta}_n = (\hat{X}_A^T \hat{X}_A)^{-1} \hat{X}_A^T y_A 
$$

$\hat{\beta}$ is then transferred to the Practitioner who can use it to compute a prediction
$$\hat{y}_V = \hat{X}_V \hat{\beta}_n $$

which will be compared to $y_V$:

$$L(\hat{y}_V, y_V) = \sum\limits_{i=1}^{n_V} (y_V^{(i)} - \hat{y}_V^{(i)})^2$$
Our end goal is to minimise this metric. 

In what we described above, the actions of the Analyst and the Practitioner are completely determined. On the other hand, we have not specified how the Imputer proceeds to the imputation. We want to investigate the effect of the choice of imputation on the expected loss:

$$R = \mathbb{E}_{\tilde{X}_V, X_A^{miss}, \epsilon_A, \epsilon_V}[(y_V^{(i)} - \hat{y}_V^{(i)})^2 \vert X_A]$$ %\footnote{Even though what we ultimately want is a decision rule for $\phi$ and $\psi$, they are only a function of the observed data $X_A$, $X_V$, which is fixed here. For simplicity of notation, we write $\phi$ and $\psi$ as constant values}

			\subsubsection{Distribution hypotheses}
Lastly, for this last expression to have any meaning, fix the distribution of $\tilde{X}$ the true data.

We assume that $X \sim \pi$ where the lines of $X$ are independent and identically distributed (i.i.d), and that $\pi$ is known to the Imputer. This is unlikely in a real setting, but we explore the best-case scenario where we have all the necessary information to perform the imputation in order to isolate the error terms that are specific to the presence of missing data --- as opposed to bad imputation.

	\section{Analysis}
		\subsection{Expected loss}
To be able to estimate the expected loss, we break it up into several components. We first denote 
$$
\tilde{\beta}_n = (\tilde{X}_A^T \tilde{X}_A)^{-1} \tilde{X}_A^T y_A 
$$

the estimated parameter we would obtain if the training data were completely observed. We consider the loss for the $i^{\text{th}}$ line of validation data $x_i^V$:

\begin{align*}
L_i(y_V, \hat{y}_V) &= (y_V - \hat{y_V})^2 &\\
				&= (\tilde{x}_i^V \beta + \epsilon_V - \hat{x}_i^V \hat{\beta}_n)^2 &\\
				   &= (\tilde{x}_i^V(\beta - \tilde{\beta}_n) + \tilde{x}_i^V (\tilde{\beta}_n - \hat{\beta}_n) + (\tilde{x}_i^V - \hat{x}_i^V) \hat{\beta}_n + \epsilon_V)^2 & \\
				   &= (\tilde{x}_i^V (\beta - \tilde{\beta}_n))^2 & (1) \\
				   & \quad + (\tilde{x}_i^V (\tilde{\beta}_n-\hat{\beta}_n))^2 &(2) \\
				   & \quad + ((\tilde{x}_i^V - \hat{x}_i^V) \hat{\beta}_n)^2 &(3) \\
				   & \quad +2 \tilde{x}_i^V (\beta - \tilde{\beta}_n) \tilde{x}_i^V (\tilde{\beta}_n - \hat{\beta}_n) & (4) \\
				   & \quad +2 \tilde{x}_i^V (\beta - \tilde{\beta}_n) (\tilde{x}_i^V - \hat{x}_i^V )\hat{\beta}_n & (5) \\
				   & \quad +2 \tilde{x}_i^V (\tilde{\beta}_n - \hat{\beta}_n) (\tilde{x}_i^V - \hat{x}_i^V) \hat{\beta}_n & (6)\\
				   & + \epsilon_V^2 &\\
				   & + \epsilon_V C
\end{align*}

Where $C$ is some term that will not matter (since it will not count in the expectation --- $\epsilon_V$ has zero expectation and is independent of the other terms). We can see that terms $(1), (2)$ and $(4)$ depend only on the imputation of the training values ($\hat{x}^V$ is absent), while term $(3)$ depends only on the imputation of the validation data ($\hat{\beta}_n$ is absent). Terms $(5)$ and $(6)$ show the interaction between both imputations.

\paragraph{Influence of missing validation values}

In particular,
\begin{proposition}
\label{prop.linear}
The expected validation error depends linearly on the proportion of missing data in the validation set (with a fixed amount of missing data in the training set).
\end{proposition}
\begin{proof}
The risk we want to minimize is $R_i$ the expectation of this loss, summed over all rows. Since the rows are i.i.d, (so the order does not count), the expected value of terms $(1),(2),(4)$ is the same for all $i$. For terms $(3), (5), (6)$ there are two possibilities: if there is no missing data in the row, these terms are zero. If a value is missing, they are nonzero but their expectation is the same for all lines with missing data. 

Consequently, denoting $r_V = \frac{k_V}{n_V}$ the proportion of missing validation data, we can express the expected loss as:

\begin{align}
\label{eq.loss}
\sum\limits_{i=1}^{n_V} L_i = &\underbrace{\mathbb{E}_{\tilde{X_V}, X_A^{miss}, \epsilon_A, \epsilon_V}[(1) + (2) + (4)\vert X_A]}_{A} + \\ 
r_V& \underbrace{\mathbb{E}_{\tilde{X_V}, X_A^{miss}, \epsilon_A, \epsilon_V}[(3) + (5) + (6)\vert X_A]}_{B} + \sigma^2
\end{align}

Thus, for $X_A$ fixed and for a given imputation rule, the expected loss is $A + \sigma^2 + Br_V$ with $A$ and $B$ fixed, and the expected loss depends linearly on the proportion of missing values. 
\end{proof}

%\begin{proposition}
%\label{prop.increase}
%If $\tilde{\beta}$ and $\hat{\beta}$ are consistent, and $\beta_1 \neq 0$, then for $n_A$ large enough the error increases with the proportion of missing validation data.
%\end{proposition} 
%\begin{proof}
%Consistency ensures that $\norm{(\beta-\hat{\beta}}$ and $\norm{(\beta-\tilde{\beta}}$ can be made arbitrarily small by increasing $n$. Using the Cauchy-Schwarz inequality, this implies terms $(5)$ and $(6)$ can be arbitrarily small, and so (using Jensen's inequality) can the norm of their expectaion.

%In addition:

%\begin{align*}
%\mathbb{E}((3)) &=\mathbb{E}[((\tilde{x}_i^V - \hat{x}_i^V) \hat{\beta}_n)^2] \\ 	
%			&\ge  \mathbb{E}[((\tilde{x}_i^V - \hat{x}_i^V) \hat{\beta}_n)]^2 \\
%			&=  (\mathbb{E}[(x_1^V - \hat{x_1^V})\mathbb{E}[\hat{\beta}_{n1}])^2
%\end{align*}

%Where $(x_1^V - \hat{x_1^V})$ is nonzero if the observation is missing and $\hat{beta}_{n1}$ can be made arbitrarily close to $\beta_1$ by increasing n (so in particular, nonzero). This means that for $n$ large enough, term $(3)$ dominates $B$ and is positive so $B >0$.
%\end{proof}

\paragraph{Consistency}
In linear regression without missing data, the estimation of the parameter is consistent\cite{consistency_linreg}, that is in our case $\tilde{\beta}_n$ is a consistent estimate of $\beta$. Moreover, Little \cite{little1992missingX} studied parameter estimation with missing values and showed:

\begin{proposition}
If the missing data is MCAR and the imputed values are the expected values of the unobserved data conditioned on the observed data ($\hat{x} = \mathbb{E}[x \vert x_{\text{obs}}$), then the least-square estimator $\hat{\beta}_n$ is consistent for $\beta$.
\end{proposition}

Note that to do this means the Imputer must know the distribution of $X$, in order to find the expected values.

If $\tilde{beta}, \hat{\beta}$ are consistent, then all terms but $(3) + \epsilon_V^2$ in the loss tend to 0 when $n\rightarrow \infty$. Additionally, term $(3)$ is zero for lines without missing data, and for lines with missing data its expectation is:
$$ \mathbb{E}[((\tilde{x}_i^V - \hat{x}_i^V) \hat{\beta}_n)^2] = \mathbb{E}[(\hat{\beta}_n^{(2)})^2] \mathbb{E}[(x_{i2}^V - \hat{x}_{i2}^V)^2] $$

Which is minimized by the choice of imputation $\hat{x}_{i2}^V = \mathbb{E}[x_{i2}^V]$, i.e.\ the same that ensures a consistent estimator for $\beta$. 
		\subsection{Consequences}
These results highlight the asymmetry between training and validation data. In a very simple case, it gives us an intuition of the behaviour of the prediction error with missing data.

We see that when the imputation is done well, consistency results ensure that in the training data, it is ultimately possible to make up for the unobserved values and have an arbitrarily good parameter estimate, if we have enough observations.

On the other hand, missingness in the validation set adds some inevitable terms to the error. This is important because although training and validation data have the same distribution by hypothesis, in some cases the missingness could be somehow different between those datasets. 

For instance, in the Traumabase it is possible that missing data comes from doctors who fail to record some values in the database, although the data was available when treating the patient. In that case when real-world prediction are made, there may be less missing values than there were when we performed CV and model selection. On the contrary, maybe some values were collected late in the process and would have been unavailable for the prehospital diagnostic. Proposition \ref{prop.linear} shows that this matters a lot, and that efforts to reduce the amount of missing data at the time of real-world predictions may have a more direct effect on performance than efforts to reduce missing data in the database (e.g.\ by improving recording). 

Lastly, presence of term $(3)$ even for large $n$ and consistent estimation also shows that the intrisic variability of the covariates imposes a limit on the performance of the prediction with missing data (while otherwise the only limit would be linked to the noise $\epsilon$ in the response). 

Still, we see that in this case at least, what could seem like independent goals (imputation and full-data parameter estimation) are actually compatible: if we try to minimize error term that depends only on imputation (using imputation by the conditional mean), then we are also enabling consistent parameter estimation which makes the other error terms tend to zero.

In the next chapter, we take a more empirical approach to investigate similar issues.