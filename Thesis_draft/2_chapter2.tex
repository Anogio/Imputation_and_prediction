	\section{Missing data mechanisms}
	Data may be missing for various reasons, and this leads to various patterns of missing data: in particular, the relationship between the missingness of observations and the true values. 
	
	Given some data $X$, we denote by $M$ the missingness indicator matrix: its coefficients are 1 if the corresponding value is missing in $X$ and 0 otherwise. We call $f(M \vert X, \phi)$ the distribution of $M$ given the data and some unknown parameters. The missingness patterns can be classified into three categories \cite[Ch.\ 1]{Rubin_missdata}:
	
\begin{itemize}
	\item \emph{Missing completely at random (MCAR):} The missingness of any given value is independent of the values of the data
	$$ f(M \vert X, \phi) = f(M \vert \phi)$$
	\item \emph{Missing at random (MAR):} The missingness depends only on the values of X that are observed and not on the missing ones
	$$f(M \vert X, \phi) = f(M \vert X_{\text{obs}}, \phi)$$
	An example would be a survey on income where we know the age of respondent, and younger people fail to declare their income more often.
	\item \emph{Missing not at random (MNAR):} The missingness depends on the values that are missing. In the same survey example, this would occur if richer people fail to declare their income more often.
\end{itemize}

In particular, Rubin showed \cite{rubin1976ignorable} that if the data is MAR or MCAR, then the likelihood factorises so that maximum-likelihood estimates can be computed by maximizing just the observed likelihood.

	\section{Main types of imputation}
		\subsection{Joint maximum likelihood}
The most straightforward way to impute data is to assume that the data is distributed according to some parametric joint distribution on all of the variables. In that case, once the distribution has been chosen, one needs to estimate the distribution parameters, and it is then possible to replace missing values by their expected value conditional on the observed data (or draws from this distribution for multiple imputation, see below).\cite{kropko2014joint} \cite{ref_amelia} \cite{pkg_norm} 

		\subsection{Fully conditional specification (FCS)}
In FCS rather than a joint model we define $p$ conditional models $\pi_1, \ldots, \pi_p$ where $\pi_i$ gives the distribution of variable $i$ conditional on the others. We can then obtain an imputed dataset iteratively using an iterative algorithm \cite{MICE_founding}.
\begin{algorithm}[H]
	\caption{FCS Algorithm}
	\hspace*{\algorithmicindent} \textbf{Input:} $X, \pi_1, \ldots, \pi_p$\\
 	\hspace*{\algorithmicindent} \textbf{Output:} $\hat{X}$
	\begin{algorithmic}[1]
		\State $\hat{X} \leftarrow $ plausible imputation of the missing data (e.g.\ mean imputation)

		\While {not converged}
			\For{$i=1 \ldots p$}
				\State $X^{(i)} \leftarrow $ the $i^{\text{th}}$ column of $X$
				\State $\hat{X}^{(-i)} \leftarrow \hat{X}$ without its $i^{\text{th}}$ column
				\State $\hat{X}^{(i)}_{\text{miss}} = \max_{\alpha} P(\alpha \vert {X}^{(i)}_{\text{obs}}, \hat{X}^{(-i)})$
			\EndFor
		\EndWhile
	\end{algorithmic}
\end{algorithm}

The interest of this approach is that is can be very flexible when the variables have very different distribution profiles. It can be used with a number of univariate conditional models \cite{MICE_founding}\cite{stekhoven2015missforest}\cite{van2007multiple}.

		\subsection{Low-rank approaches}
An alternative to assuming some distribution for the dataset is to find a low-rank representation of the data and use it to impute unobserved values. Such approaches \cite{josse2012missPCA}\cite{chen2004missSVD}\cite{brand2002incremental} are generally based on Principal Component Analysis (PCA)\cite{PCA}, using the iterative PCA algorithm \cite{iter_PCA}:		
		\begin{algorithm}[H]
	\caption{Iterative PCA Algorithm}
	\hspace*{\algorithmicindent} \textbf{Input:} $X, k$ \\
 	\hspace*{\algorithmicindent} \textbf{Output:} $\hat{X}$
	\begin{algorithmic}[1]
		\State $\hat{X} \leftarrow $ plausible imputation of the missing data (e.g.\ mean imputation)
		\While {not converged}
			\State $V \leftarrow k$ first principal components of $\hat{X}$ (complete dataset)
			\State $\tilde{X} \leftarrow \hat{X}$ projected on the span of $V$ (i.e.\ PCA fitted values)
			\State $\hat{X} \leftarrow \hat{X} * (1-M) + (\tilde{X} * M$ where $M$ the missingness indicator
		\EndWhile
	\end{algorithmic}
\end{algorithm}		

That is, the missing values are repeatedly imputed by projection on the principal components. 
		
		\subsection{Nearest-neighbors}
Other methods exist to impute missing values. \emph{Nearest-neighbor imputation}\cite{chen2000nearest}consists in replacing missing values for one individual by the values observed in similar individuals. Using the observed value, a distance metric is computed with the other observations in the data, and we pick the closest one which has observations in the variable we need to impute. This value is then used for imputation.

 The related \emph{hot-deck imputation} \cite{andridge2010hotdeck} use multiple similar individuals to generate imputed values. Rather than pick a single closest value, we can pick a pool of similar individuals and draw at random between their observed values, or impute via a combination of those values.

	\section{Multiple imputation}
		\subsection{Principle}
One major drawback of imputation is that it hides the difference between values that were really observed and those that were initially missing. If one uses full-data analysis on an imputed dataset, the results may be overconfident, and in particular underestimate variances because the uncertainty from missing data is not taken into account.

In order to retain the advantages of imputation (using any complete-data method) while compensating for this overconfidence, Rubin \cite{rubin1986mi_founding} introduced \emph{multiple imputation}. The idea is that rather than just one dataset, one should generate $m$ plausible imputed values for each missing observation, in order to account for the uncertainty of the imputation. By performing her inference on each imputed dataset, the Analyst then obtains a set of estimates rather than just one. 

When the imputation is based on a distribution, one can use draws from the distribution rather than the expected mean as one would do for single imputation. In other cases, method-specific approaches have to be designed.

Once the datasets are generated, and estimates have been computed, it is possible to combine them to obtain a new estimation for the variances of our estimates\cite[Ch.\ 5]{Rubin_missdata}.

		\subsection{Rubin's rule for result aggregation}

Let us denote $(\hat{\theta}_1, \ldots, \hat{\theta}_m)$ the $m$ estimates for a given parameter $\theta$, and $W_1, \ldots, W_m$ the variance for $\theta$ estimated by the complete-data method (within-imputation variance). The aggregated estimate for $\theta$ is

$$\hat{\theta} = \frac{1}{m} \sum\limits_{i=1}^m \hat{\theta}_i $$

And the aggregated within-imputation variance 

$$ \hat{W} = \frac{1}{m} \sum\limits_{i=1}^m W_i $$

The between-imputation variance can be computed as the sample variance of the estimates:

$$\hat{B} = \frac{1}{m-1} \sum\limits_{i=1}^m (\hat{\theta}_i - \hat{\theta})^2$$

Then the total variability associated to $\theta$ is then \cite[Ch.\ 5]{Rubin_missdata}:
$$\hat{T} = \hat{W} + \frac{m+1}{m} \hat{B}$$

Moreover, if $\theta$ is a scalar, when $n \rightarrow \infty$ we can approximate \cite{rubin1986mi_founding}
$$ (\theta - \hat{\theta})\hat{T}^{-\frac{1}{2}} \sim t_\nu$$

where $t_\nu$ is a t-distribution with $\nu = (m-1)(1+\frac{1}{m+1}\frac{\hat{W}}{\hat{B}})^2$. 

Using this approximation, it is possible to compute confidence intervals for $\theta$ that take into account the uncertainty related to missing data.
