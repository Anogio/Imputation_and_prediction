```{r setup, include=FALSE}
library(tidyverse)
library(MLmetrics)
library(mice)
library(Amelia)
library(missForest)
library(missMDA)
```

```{r}
aux.folder = '../../auxiliary/'
source(paste(aux.folder,'MVN_imputation.R',sep=''), chdir = T)
source(paste(aux.folder,'generate_missing.R',sep=''), chdir = T)
source(paste(aux.folder,'prediction_methods.R',sep=''), chdir = T)
source(paste(aux.folder,'simulation_methods.R',sep=''), chdir = T)
data_folder = '../../../Data/'

criteria = read.csv(paste(data_folder, 'criteria.csv', sep=''), row.names=1)
criteria$HS.procedure = NULL

seed = ceiling(runif(1,1e3,1e5))
print(seed)
seed.run = seed
``` 

```{r}
oneRun = function(args){
  for(name in names(args)){
    assign(name, args[[name]])
  }
  errWeight = F
  if(exists('seed.run')){
    set.seed(seed.run)
    print('seeded')
    print(seed.run)
  }
  
  if(errFun=='AUC'){
    errFun = AUC
  }
  else if(errFun=='weighted'){
    errFun = function(y.pred,y.true){metric_best_separation(y.pred,y.true, positive_weighting=pos.weight)$val}
    errWeight = T
  }else if(errFun=='logloss'){
    errFun = MLmetrics::LogLoss
  } else{
    stop('Wrong error function')
  }
  

  spl = train_test_split(X, y, train_prop)
  inTrain = spl$inTrain
  y.train = y[inTrain]
  y.test = y[-inTrain]
    
  datasets = lapply(filled.datasets, function(x){print(dim(x));list(train=x[inTrain,useCols.pred], test=x[-inTrain,useCols.pred])})
  
  regressors.fit = lapply(datasets,
                          function(x) {(predictor$train)(as.matrix(x$train), y.train)})
  
  predictions = lapply(names(datasets), function(x){(predictor$predict)(regressors.fit[[x]], as.matrix(datasets[[x]]$test))})
  names(predictions) = names(datasets)
  errors = lapply(predictions, function(x){errFun(x,y.test)})
  return(c(errors, list(beta=regressors.fit$MVN$coefficients)))
}
```

```{r}
args = list(train_prop=0.7, n=NULL, pos.weight=5)

X.gen = X.trauma
y.gen = y.trauma
#predictor = reg.logit
predictor = reg.logit

X = X.gen(args)
y.g = y.gen(X,args)

y = y.g$y
X = y.g$X %>% as.data.frame()
X.cat = y.g$X.aux
X$Sexe = as.numeric(X.cat$Sexe)

X = MCAR.noEmptyLines(X,0.2)

useCols.pred = c("Sexe", "Age", "BMI", "Glasgow.initial", 
"Hemocue.init", "SpO2.min", "Remplissage.total.cristalloides", 
"Remplissage.total.colloides", 'SD.min', 'FC.max'
)
#X = X[,useCols.pred]
 

filled.datasets = list()
  
filled.datasets[['mice.pmm']] = mice::complete(mice(X, m=1, method='pmm', printFlag = F))
filled.datasets[['mice.norm']] = mice::complete(mice(X, m=1, method='norm', printFlag = F))
X.cust = X[,useCols.pred]
X.cust$Glasgow.initial = as.factor(X.cust$Glasgow.initial)
X.cust$Sexe = as.factor(X.cust$Sexe)
#X.cust$Remplissage.total.cristalloides = as.factor(X.cust$Remplissage.total.cristalloides)
#X.cust$Remplissage.total.colloides = as.factor(X.cust$Remplissage.total.colloides)
filled.datasets[['mice.custom']] = mice::complete(mice(X.cust, m=1, method=c('logreg','pmm', 'pmm', 'polr', 'pmm', 'pmm',    'pmm', 'pmm','pmm','pmm'), printFlag = F)) %>% lapply(as.numeric) %>% as.data.frame()
filled.datasets[['mean']] = mice::complete(mice(X, m=1, method='mean', printFlag = F))
filled.datasets[['mipca']] = MIPCA(X, ncp=3, nboot=1)$res.MI[[1]]
filled.datasets[['amelia']] = amelia(X, m=1, noms=c(), p2s=0, parallel = 'multicore', ncpus = 4)$imputations$imp1
#filled.datasets[['MF']] = missForest(X)$ximp
imp.mvn.trained = imp.mvnorm.train(X)
filled.datasets[['MVN']] = imp.mvnorm.estim(imp.mvn.trained, X)
```


```{r}
errFun = 'weighted'

S = 128
res = evaluate.S.run.general(S, args, evaluator=oneRun, do.parallel=T) %>% as.data.frame()
res %>% gather('method', 'error') %>%
  ggplot() + aes(x=reorder(method,error,median), y=error) + geom_boxplot()
#stop("WARNING! J'utilise SD.SMUR et FC.SMUR qui ont plein de donnÃ©es manquantes -> mieux pour allCols")
```

```{r}
r= matrix(res$beta, ncol=11, byrow=T)
colnames(r) = c('Intercept', useCols.pred)
beta_exp = colMeans(r)

typical_size = c(1, 1, 30, 50, 10, 15, 100,1000, 1000, 50, 100)
contrib = beta_exp * typical_size
contrib/mean(contrib)
```

