---
title: "Comparison of aggregation methods for multiple imputation"
output: html_document
---

In this document, the goal is to compare different ways of pooling multiple imputation results, and evaluate the impact of the choice of method (and the number of imputations) on the prediction.

To illustrate this, we use the Traumabase dataset. We perform 40 imputations of the missing values, and for each imputed dataset we fit a model and perform a prediction. Then, we aggregate the predictions using various methods. More precisely, we proceed as follow:

- Choose an aggregation method (e.g. to predict the value of an observation, take the average of the predictions made for this observation on each imputed dataset)
- For i from 1 to n_imputations, aggregate the predictions on the first i datasets (that is, our estimated values for 1 imputation are just the predictions on the first imputed dataset, for 2 imputations it is the mean of the first two predictions, etc... In other words, for mean aggregation, we take the cumulative average of the predictions on a growing number of datasets). This allows us to get a prediction for each number of imputations.
- We compute prediction metrics on each of the prediction. This allows us to see the evolution of the predictive performance w.r.t the number of imputation for each aggregation method.

An interesting aspect of this dataset is that it has unbalanced classes. While it makes it more difficult to find a good metric for the prediction, it allows us to demonstrate some advantages of multiple imputation. In our case, positives are patients who are in a critical state, so false negatives are much more costly than false positives. The imbalance of the classes and asymmetry of cost means that it is interesting to have access to a measure of uncertainty on the prediction. By having access to multiple predictions (taking into account the variability linked to missing data) can choose to be more conservative when aggregating those predictions. There are many ways to do this, we kept two in this example:

- Take the third quartile of the prediction. This is a robust estimator which should consistently give predictions that are biased towards positive cases, without sacrificing too much accuracy.
- Take the mean of the predictions and shift it by one standard deviation (meaning, standard deviation of the multiple predictions on a given observation). Though promising, we see below that this method tends to overpredict positives so much that is loses much of its use.

The final results does not show the quartile aggregation performs any better than both the mean and median aggregations (which would be the standard go-to in most scenarios). However, when we add some more missing values artificially (not shown here), it becomes quite significantly better.

It is also important to note that there is a noticeable increase in performance between just one imputation and 5-10 imputations, whichever aggregation method is chosen. 

NB: Here, we performed prediction using only logistic regressions. The main reason is that, in order to avoid confusing the results, we chose to pick a prediction method that is known to need small amount of data, and would not benefit from multiple training rounds in any obvious way. With a random forest, for instance, averaging out multiple predictions could be seen as performing just one random forest with twice as many estimators.

NB2: The 'Separation metric' used at the end of the document proceeds as follow: for a given prediction thresholds (the value above which we predict a positive), the loss is estimated as a weighted sum of the false positives and the false negatives (the weight is chosen by hand). The separation metric is the lowest loss that can be achieved by choosing the right threshold. As such, it gives an idea of the best separation between the classes that can be achieved. It is complementary to the AUC which measures the overall separation, but which can sometimes be misleading in cases where the classes are unbalanced.

## Miscellaneous setup
Imports
```{r, results="hide", message=FALSE}
## Imports
library(tidyverse)
library(verification)
library(missMDA)
library(FactoMineR)
# Custom imports
aux.folder = '../../auxiliary/'
source(paste(aux.folder,'dataloaders.R',sep=''), chdir = T)
source(paste(aux.folder,'generate_missing.R',sep=''), chdir = T)
source(paste(aux.folder,'imputation_methods.R',sep=''), chdir = T)
source(paste(aux.folder,'prediction_methods.R',sep=''), chdir = T)

data_folder = '../../../Data/'
```

Parameters for this run
```{r}
#Global seed
seed = 2
# Dataset to use for the analysis
dataset = 'trauma'
# Number of datasets generated via multiple imputation
n_imputations = 40
# Prediction method used on the competed datasets
prediction_method = 'logit'
# Proportion of observations used to train the models (the rest is used for evaluation)
train_size = 0.4
# Maximum number of rows to keep in the dataset. Keep all rows if NULL
max_rows = NULL

prop_added_missing = 0

keep_data = 'num' # all, cat or num
imputation_methods = c('mi', 'mice', 'amelia', 'mean') #any method accepted by the *impute* method from prediction_methods.R


set.seed(seed)
```

```{r}
# Rows of the trauma dataset to keep. The first set of columns is for the numeric variables as in Wei's work. The second is 
# from an empirical evaluation of useful features for the prediction
keepCols_num = c("Age", "Glasgow.moteur.initial", "FC.max", "Hemocue.init",
                 "Remplissage.total.cristalloides","Remplissage.total.colloides", "SD.min", "SD.SMUR")
keepCols_all = c("SD.min", "FC.max", "Hemocue.init", 
                     "Age", "BMI", "Catecholamines", "Remplissage.total.cristalloides", 
                     "Poids", "Remplissage.total.colloides", "Taille", "FC.SMUR", 
                     "SD.SMUR", "TC.incarceration", "SpO2.min", "TA.ischemie", "TA.amputation", 
                     "Glasgow.initial", "TC.ecrase.projete", "Glasgow.moteur.initial", 
                     "TC.chute")
```

## Data preparation
Import and select columns

```{r}
# Load and format the dataset
dat = loader(dataset, max_rows, seed)
X = dat$X_numeric
X = X %>% dplyr::select(one_of(keepCols_num))
y = dat$y

X = MCAR(X, prop_added_missing)

print(paste('X is of dimension', toString(dim(X))))
```

```{r}
splitted = train_test_split(X,y,train_size,seed=seed)
spl = splitted$spl

X_fill_MI = impute(X, 'mice', m=n_imputations)
y_pred_MI = multiple_prediction(X_fill_MI, y, pred_method = prediction_method, 
                                  train_size = train_size, seed=seed, spl=spl)
```
```{r}
res = data.frame(y_pred_MI$y_pred)
print('Performing PCA on the multiple predictions')
p = PCA(res)
```
```{r, results='hide'}
preds_pool_mean = matrix(NA, nrow=length(y)-length(spl), ncol=n_imputations)
preds_pool_median = matrix(NA, nrow=length(y)-length(spl), ncol=n_imputations)
preds_pool_mean.biased = matrix(NA, nrow=length(y)-length(spl), ncol=n_imputations)
preds_pool_quartile = matrix(NA, nrow=length(y)-length(spl), ncol=n_imputations)

preds_pool_mean[,1] = y_pred_MI$y_pred[[1]][,2][-spl]
preds_pool_median[,1] = y_pred_MI$y_pred[[1]][,2][-spl]
preds_pool_mean.biased[,1] = y_pred_MI$y_pred[[1]][,2][-spl]
preds_pool_quartile[,1] = y_pred_MI$y_pred[[1]][,2][-spl]
  
for(i in 2:n_imputations){
  print(i)
  preds_pool_mean[,i] = pool_MI_binary(preds=list(y_pred=y_pred_MI$y_pred[1:i]), spl=spl, method = 'mean')
  preds_pool_median[,i] = pool_MI_binary(preds=list(y_pred=y_pred_MI$y_pred[1:i]), spl=spl, method = 'quantile', quantile.split = 0.5)
  preds_pool_quartile[,i] = pool_MI_binary(preds=list(y_pred=y_pred_MI$y_pred[1:i]), spl=spl, method = 'quantile', quantile.split = 0.75)
  preds_pool_mean.biased[,i] = pool_MI_binary(preds=list(y_pred=y_pred_MI$y_pred[1:i]), spl=spl, method = 'biased.mean')
 }
```

```{r}

f =  function(x){roc.area(as.numeric(y[-spl])-1,x)$A}

Losses = data.frame(n_imputations=1:n_imputations)
Losses$mean = apply(preds_pool_mean, 2, f)
Losses$median = apply(preds_pool_median, 2, f)
Losses$mean.biased = apply(preds_pool_mean.biased, 2, f)
Losses$quartile = apply(preds_pool_quartile, 2, f)
Losses %>% gather('Pool.method', 'Loss', -n_imputations) %>%
  ggplot() + aes(x=n_imputations, y=Loss, color=Pool.method) + geom_line() +
  ylab('AUC')
```

```{r}
f = function(x){metric_best_separation(x,y[-spl], 10)$val}
Losses = data.frame(n_imputations=1:n_imputations)
Losses$mean = apply(preds_pool_mean, 2, f)
Losses$median = apply(preds_pool_median, 2, f)
Losses$mean.biased = apply(preds_pool_mean.biased, 2, f)
Losses$quartile = apply(preds_pool_quartile, 2, f)

Losses %>% gather('Pool.method', 'Loss', -n_imputations) %>%
  ggplot() + aes(x=n_imputations, y=Loss, color=Pool.method) + geom_line() +
  ylab('Separation metric')
```

