---
title: "Comparison of various imputation methods for the prediction of haemorrhagic shock (numerical data only)"
output: html_document
---
  
## Miscellaneous setup
Imports
```{r, results="hide", message=FALSE}
## Imports
library(tidyverse)
library(verification)

# Custom imports
aux.folder = '../../auxiliary/'
source(paste(aux.folder,'dataloaders.R',sep=''), chdir = T)
source(paste(aux.folder,'generate_missing.R',sep=''), chdir = T)
source(paste(aux.folder,'imputation_methods.R',sep=''), chdir = T)
source(paste(aux.folder,'prediction_methods.R',sep=''), chdir = T)

data_folder = '../../../Data/'
```

Parameters for this run
```{r}
#Global seed
seed = 2
# Dataset to use for the analysis
dataset = 'trauma'
# Proportion of MCAR mising data added to the dataset
prop_added_missing = 0
# Number of datasets generated via multiple imputation
n_imputations = 2
# Prediction method used on the competed datasets
prediction_method = 'rf'
# Proportion of observations used to train the models (the rest is used for evaluation)
train_size = 0.8
# Maximum number of rows to keep in the dataset. Keep all rows if NULL
max_rows = NULL

keep_data = 'num' # all, cat or num
imputation_methods = c('mice', 'mean', 'mipca') #any method accepted by the *impute* method from prediction_methods.R

# Do we perform xgboost prediction on the non-imputed dataset?
pred_xgb = T
nrounds_xgb = 100

# Do we perform SAEM prediction on the non-imputed dataset? (continuous data only)
pred_SAEM = T

set.seed(seed)
```

```{r}
# Rows of the trauma dataset to keep. The first set of columns is for the numeric variables as in Wei's work. The second is 
# from an empirical evaluation of useful features for the prediction
keepCols_num = c("Age", "Glasgow.moteur.initial", "FC.max", "Hemocue.init",
                 "Remplissage.total.cristalloides","Remplissage.total.colloides", "SD.min", "SD.SMUR")
keepCols_all = c("SD.min", "FC.max", "Hemocue.init", 
                     "Age", "BMI", "Catecholamines", "Remplissage.total.cristalloides", 
                     "Poids", "Remplissage.total.colloides", "Taille", "FC.SMUR", 
                     "SD.SMUR", "TC.incarceration", "SpO2.min", "TA.ischemie", "TA.amputation", 
                     "Glasgow.initial", "TC.ecrase.projete", "Glasgow.moteur.initial", 
                     "TC.chute")
```

## Data preparation
Import and select columns

```{r}
# Load and format the dataset
dat = loader(dataset, max_rows, seed)
y = dat$y

# Keep relevant columns
if(keep_data == 'all'){
  X = cbind(dat$X_numeric, dat$X_category)
  if(dataset=='trauma'){
    x = data.frame(Discordance=X$Discordance, Hemo.shock=as.numeric(y)-1)
    print('Illustrate weird Discordance association')
    print(plyr::count(x))
  
    X = X %>% dplyr::select(-Discordance) # Discordance has a strange correlation with y, it is removed (illustrated above)
    X = X %>% dplyr::select(one_of(keepCols_all)) 
  }
}else if(keep_data == 'cat'){
  print('Keeping only categorical variables')
  X = dat$X_category
}else if(keep_data=='num'){
  print('Keeping only numerical variables')
  X = dat$X_numeric
  if(dataset=='trauma'){
    X = X %>% dplyr::select(one_of(keepCols_num))
  }
}else{
  stop('wrong keep_data value')
}

print(paste('X is of dimension', toString(dim(X))))
```

Add some missing values (or not)
```{r}
X_miss = MCAR(X, prop_added_missing)
```

## Prediction
Perform (multiple) imputation and prediction for every selected method
```{r, message=F, warning=F}
# Choose the train-test split
splitted = train_test_split(X_miss,y,train_size,seed=seed)
spl = splitted$spl
y_true = splitted$y_test

y_pred = list()
trained_predictors = list()
times = list()

X_miss = rbind(X_miss,
          oversample(X_miss[spl,], y[spl], 8)
)
added = nrow(X_miss)-length(y)
spl = c(spl, (nrow(X)+1):(nrow(X_miss)))
y = unlist(list(y,as.factor(base::rep('X1',added))))

for(method in imputation_methods){
  t0 = Sys.time()
  # Impute the missing data
  X_imputed = impute(X_miss, m=n_imputations, method=method)
  # Perform prediction on the filled dataset
  y_pred_multi = multiple_prediction(X_imputed, y, pred_method = prediction_method, 
                                  train_size = train_size, seed=seed, spl=spl)
  # Pool the results to get a single estimate
  if(method == 'mean'){
    name = paste(method, '+', prediction_method)
    y_pred[[name]] = y_pred_multi$y_pred[[1]][-spl,2]
  }
  else{
    name = paste(method,'(', n_imputations, ')', '+', prediction_method)
    y_pred[[name]] = pool_MI_binary(y_pred_multi, spl, method='mean', y_true=y)
  }
  
  # Store additional info
  trained_predictors[[method]] = y_pred_multi$trained_predictors
  
  dt=as.numeric(Sys.time()) - as.numeric(t0)
  times[[name]] = dt
  print(paste('Finished in', dt, "seconds."))
  print('')
}

if(pred_xgb){
  # Perform XGBoost prediction
  t0 = Sys.time()
  y_pred_xgb = xgboost_prediction(X_miss, y, train_size=train_size, seed=seed, nrounds = nrounds_xgb, spl=spl)
  y_pred[['xgb']] = y_pred_xgb$y_pred[-spl]
  trained_predictors[['xgb']] = y_pred_xgb$trained.predictor
 
  t=as.numeric(Sys.time()) - as.numeric(t0)
  times[['XGBoost']] = t
  print(paste('Finished in', t, "seconds."))
  print('')
}
if(pred_SAEM){
  # Perform SAEM prediction
  if(keep_data=='num'){
    t0 = Sys.time()
    y_pred[['SAEM']] = saem_prediction(X_miss, y, train_size=train_size, seed=seed, printevery=50, spl=spl)$y_pred[-spl] 
    
    t=as.numeric(Sys.time()) - as.numeric(t0)
    times[['SAEM']] = t
    print(paste('Finished in', t, "seconds."))
  }
  else{
    stop('SAEM only works on continuous data')
  }
}
```

## Result aggregation and comparison
Compute the average predictor for multiple imputation and regroup all the predictions in a table
```{r}
results = data.frame(y_pred)
results$Hemo.shock = as.numeric(y_true == 'X1')

print('Estimations of the probability of hemorragic shock by various methods')
head(results)
```

Plot the ROC curve for these estimators. The number in the legend is the AUC (are under the ROC curve) for each model.
```{r, warning=FALSE}
roc.plot(results$Hemo.shock, results[,1:(length(y_pred))], legend=T, leg.text=names(y_pred))
```

In the case with only numeric data, all methods have pretty much the same performance. It is interesting to note that when we restrict the dataset to 1000 entries, the SAEM logistic regression performs significantly better than the other methods. 

In the case with both numeric and categorical data, the results are significantly better (AUC around 0.87 rather than 0.83), but all methods still have almost identical performance.

Below is the importance of the variables used by the classifier (for the mean-imputed dataset)

```{r}
importances = varImp(trained_predictors[['mice']][[1]])
imp = data.frame(importances$importance)
colnames(imp) = c('MICE')
imp$Mean = varImp(trained_predictors[['mean']][[1]])$importance$Overall
#imp$Amelia = varImp(trained_predictors[['amelia']][[1]])$importance$Overall
imp$Missingness = colSums(is.na(X_miss)) / nrow(X_miss)*100
imp
```

```{r}
timesdf = data.frame(method=names(times), timing = unlist(times))
ggplot(timesdf) + aes(x=method, y=timing) + geom_bar(stat='identity')
```
```{r}
thresh = 0.5
i=5
print(colnames(results[i]))
print(confusionMatrix(factor(as.integer(results[,i]>thresh)),as.factor(results$Hemo.shock), positive='1'))
print(LogLoss(as.numeric(results[,i]), results$Hemo.shock))
print(weighted_log_loss(y_pred=as.numeric(results[,i]), y_true=results$Hemo.shock, y_train=splitted$y_train))
```

