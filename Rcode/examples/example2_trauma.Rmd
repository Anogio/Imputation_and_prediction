---
title: "Comparison of SAEM to multiple imputation methods on trauma data"
output: html_document
---

## Miscellaneous setup
Imports

```{r, results="hide", message=FALSE}
## Imports
library(tidyverse)
library(verification)

# Custom imports
aux.folder = '../auxiliary/'
source(paste(aux.folder,'dataloaders.R',sep=''), chdir = T)
source(paste(aux.folder,'generate_missing.R',sep=''), chdir = T)
source(paste(aux.folder,'imputation_methods.R',sep=''), chdir = T)
source(paste(aux.folder,'prediction_methods.R',sep=''), chdir = T)
```

Parameters for this run
```{r}
seed = 42
dataset = 'trauma'
prop_added_missing = 0.05
n_imputations = 5
prediction_method = 'rf'
train_size = 0.5
max_rows = NULL
```

## Data preparation
Import and select columns

```{r}
# Load and format the dataset
dat = loader(dataset, max_rows, seed)
X = dat$X_numeric # For now we keep only a subset of the given data, as in Wei's work
X = X %>% dplyr::select(Age, Glasgow.moteur.initial, FC.max, Hemocue.init, Remplissage.total.cristalloides, Remplissage.total.colloides,
                        SD.min, SD.SMUR)
y=dat$y
```

Add some missing values (or not)
```{r}
X_miss = MCAR(X, prop_added_missing)
```

## Prediction
### SAEM Prediction
```{r}
prediction_SAEM = saem_prediction(X_miss, y, train_size=train_size, seed=seed, printevery=50)
```

### Imputed prediction
Perform multiple imputation
```{r}
X_imputed = mice_imp(X_miss, m=n_imputations)
```

Perform imputaton by the mean
```{r}
X_fillmean = X_miss
for(i in 1:ncol(X_fillmean)){
  c = X_fillmean[,i]
  X_fillmean[is.na(c),i] = mean(c, na.rm = T)
}
```

Fit and predict on each filled dataset using the same train/test split as with the SAEM. Here the prediction is made using any method available for caret (the choice is made at the top of the document. Here, we use a random forest estimator with no parameter tuning.)
```{r}
predictions = multiple_prediction(X_imputed, y, pred_method = prediction_method, 
                                  train_size = train_size, seed=seed, spl=prediction_SAEM$spl)
```

Perform a single prediction on the filled dataset with the mean
```{r}
prediction_meanfill = multiple_prediction(list(X_fillmean), y, pred_method = prediction_method, 
                                          train_size = train_size, seed=seed,spl=prediction_SAEM$spl)
```

## Result aggregation and comparison
Compute the average predictor for multiple imputation and regroup all the predictions in a table
```{r}
y_pred = matrix(NA, nrow=length(predictions$y_true), ncol=n_imputations)
for(i in 1:n_imputations){
  y_pred[,i] = predictions$y_pred[[i]][,2]
}
results = data.frame(row.names=1:nrow(y_pred))
results$MI_estimate = apply(y_pred, 1, mean)
results$SAEM_estimate = prediction_SAEM$y_pred
results$meanImp_estimate = prediction_meanfill$y_pred[[1]][,2]
results$Hemo.shock = as.numeric(predictions$y_true == 'X1')

print('Estimations of the probability of hemorragic shock by various methods')
head(results)
```

Plot the ROC curve for these estimators. The number in the legend is the AUC (are under the ROC curve) for each model.
```{r, warning=FALSE}
roc.plot(results$Hemo.shock, results[,c(1,2,3)], legend=T, leg.text=c("MI + randomforest", 'SAEM prediction', "Mean imputation + randomforest"))
```

In this case, the results are extremely similar. It is interesting to note that when we restrics the dataset to 1000 entries, the SAEM logistic regression performs significantly better than the other methods. In our work to come, we will try to see how this performance discrepancy evolves when we increase the amount of missing data, as well as the performance of other imputation methods. 
