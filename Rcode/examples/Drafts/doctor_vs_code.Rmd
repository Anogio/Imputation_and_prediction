---
title: "Doctor vs algo"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret)

set.seed(1e3)
```

Here we compare the predictions made by doctors and by a very simple algorithm . To that end, we train a logistic regression on the Traumabase data and make a comparison between its predictions and the recorded prediction made by the doctors.

## Load and prepare data
```{r}
# Choose explanatory variables
hemo_columns =  c("Sexe","Age", "Glasgow.moteur.initial", "FC.max", "Hemocue.init", "PAS.min", "PAD.min", "PAS.SMUR", "PAD.SMUR", #"SpO2.min",
                 "Remplissage.total.cristalloides","Remplissage.total.colloides", "Choc.hemorragique", 'Discordance', 'Mecanisme', 'ACR.1', 'Origine')

traumdata <- read.csv(file='data_trauma.csv', header = TRUE,na.strings = c("","NR","IMP","NA","NF"),encoding = "UTF-8") %>% as.data.frame()
traumdata = traumdata[,hemo_columns]

# Remove observations with cardiac arrest or penetrating trauma, unknown sex or transferred from a different hospital
traumdata = traumdata %>% filter(ACR.1 != 1, 
                       !(Mecanisme %in% c("Arme blanche", "Arme Ã  feu")),
                       Origine == 'Primaire',
                      !is.na(Discordance),
                      !is.na(Sexe)
                      )

# Compute pulse pressure
traumdata$SD.min=traumdata$PAS.min-traumdata$PAD.min
traumdata$SD.SMUR=traumdata$PAS.SMUR-traumdata$PAD.SMUR

# Remove variables that will not be used for prediction
traumdata = traumdata %>% dplyr::select(-one_of(c("PAS.min", "PAD.min", "PAS.SMUR", "PAD.SMUR", 'ACR.1', 'Origine', 'Mecanisme')))

# Prepare the data for prediction
X = traumdata %>% dplyr::select(-one_of(c("Choc.hemorragique","Discordance")))
y = traumdata$Choc.hemorragique
y.pred.doctor = traumdata$Discordance %>% as.character()

# Split the data in two datatest: one to learn the model and one for validation
inTrain = caret::createDataPartition(y, p=0.75, list=F)

# Impute the missing data by replacing missing observations with the mean of the variable
mu = colMeans(X[inTrain,-1], na.rm=T)

for(i in 2:ncol(X)){
  c = X[,i]
  c[is.na(c)] = mu[i-1]
  X[,i] = c
}

X.train = X[inTrain,]
X.test = X[-inTrain,]
y.train = y[inTrain]
y.test = y[-inTrain]
```

## Perform the prediction

The logistic regression predicts a probability for the patient to have haemorrhagic shock. To turn this into a binary prediction, we manually choose a threshold above which we choose to predict a shock.
```{r}
# Compute a logistic regression and predict the probability of Haemorrhagic shock
regression.model = glm(y.train~., family=binomial(link = 'logit'), data=cbind(X.train, y.train))
y.pred.regression = predict(regression.model,X.test, type='response')
# Take a threshold to predict whether harmorrhagic shock will occur

threshold = 0.15
y.pred.regression[y.pred.regression<threshold] = 0
y.pred.regression[y.pred.regression>=threshold] = 1
y.pred.regression = as.factor(y.pred.regression)

# Convert the acronyms to the related prediction
y.pred.doctor[y.pred.doctor %in% c('PDSC','PGA')] = 0
y.pred.doctor[y.pred.doctor %in% c('PDAC', 'MGA')] = 1
y.pred.doctor = as.factor(as.numeric(y.pred.doctor))[-inTrain]

y.true = as.factor(y.test)
```

## Evaluate the prediction

The confusion matrix shows how patients of different types were classified by the doctors and the algorithm. To get a numeric estimation of the performance of the predictions, we assign a cost to both types of mistakes (false positives and false negatives) and compute the average cost associated with each method.
```{r}
# Compute the confusion matrix to evaluate the prediction
confusion.doctor = confusionMatrix(y.pred.doctor, y.true, positive = '1')$table
confusion.regression = confusionMatrix(y.pred.regression, y.true, positive = '1')$table
print("Confusion matrix for the doctor's prediction:")
print(confusion.doctor)
print("Confusion matrix for the algorithm's prediction:")
print(confusion.regression)

FP.cost = 1
FN.cost = 10

doctor.cost = (FP.cost * confusion.doctor[2,1] + FN.cost * confusion.doctor[1,2])/length(y.test)
regression.cost = (FP.cost * confusion.regression[2,1] + FN.cost * confusion.regression[1,2])/length(y.test)

print('Relative difference in cost between the two predictions  - when a false negative (missed shock) costs 10 times as much as a false positive (predicted shock but no actual shock):')
cat((regression.cost - doctor.cost)/doctor.cost*100, '%', sep='')
```
 As we can see, the algorithm's prediction is about 25% better than the doctor with this choice of weights. 
 
 We see on the confusion matrix that the algorithm tends to be more prone to predict haemorrhagic shock. Thus it gets both more false positives and true positives, but less false negatives. Thus, it is sensible to wonder which cost one would need to choose for false negatives (missing a shock) so that the doctors and the algorithm would get the same performance score. It is given by the formula below:
```{r}
(confusion.doctor[2,1] - confusion.regression[2,1])/(confusion.regression[1,2]-confusion.doctor[1,2])
```
If false negatives are worth less than 4 times the cost of false positives, then doctors are better (this is just an estimate of the true value, in particular it will vary slightly depending of how we split the data between training and validation samples).

Keep in mind that we could compensate a bit by changing the threshold for positive prediction, if the cost of missing a patient in shock is actually lower.
