---
  title: "Comparison of SAEM to multiple imputation methods on trauma data"
output: html_document
---
  
  ## Miscellaneous setup
  Imports

```{r, results="hide", message=FALSE}
## Imports
library(tidyverse)
library(verification)

# Custom imports
aux.folder = '../auxiliary/'
source(paste(aux.folder,'dataloaders.R',sep=''), chdir = T)
source(paste(aux.folder,'generate_missing.R',sep=''), chdir = T)
source(paste(aux.folder,'imputation_methods.R',sep=''), chdir = T)
source(paste(aux.folder,'prediction_methods.R',sep=''), chdir = T)
```

Parameters for this run
```{r}
seed = 42
dataset = 'trauma'
prop_added_missing = 0
n_imputations = 5
prediction_method = 'rf'
train_size = 0.5
max_rows = 1000
```

## Data preparation
Import and select columns

```{r}
# Load and format the dataset
dat = loader(dataset, max_rows, seed)
X = cbind(dat$X_numeric, dat$X_category)
y=dat$y

x = data.frame(Disc=X$Discordance, y=y)
print('Illustrate weird Discordance association')
print(plyr::count(x))

X = X %>% dplyr::select(-Discordance) # Weird effect of discordance

X = X %>% dplyr::select(one_of(c("SD.min", "FC.max", "Hemocue.init", 
"Age", "BMI", "Catecholamines", "Remplissage.total.cristalloides", 
"Poids", "Remplissage.total.colloides", "Taille", "FC.SMUR", 
"SD.SMUR", "TC.incarceration", "SpO2.min", "TA.ischemie", "TA.amputation", 
"Glasgow.initial", "TC.ecrase.projete", "Glasgow.moteur.initial", 
"TC.chute")))
```

Add some missing values (or not)
```{r}
X_miss = MCAR(X, prop_added_missing)
```

## Prediction

### XGBoost Prediction
```{r}

predictions_xgb = xgboost_prediction(X_miss, y, train_size=train_size, seed=seed)
```

### Imputed prediction
Perform multiple imputation
```{r}
X_imputed_mice = mice_imp(X_miss, m=n_imputations)
X_imputed_amelia = amelia_imp(X_miss, m=n_imputations)
X_imputed_mi = mi_imp(X_miss, m=n_imputations)
#X_imputed_cat = cat_imp(X_miss, m=n_imputations)
```

Perform imputaton by the mean
```{r}
catCols = which(sapply(X, is.factor))
numCols = setdiff(1:ncol(X), catCols)
X_fillmean = X_miss
for(i in numCols){
  c = X_fillmean[,i]
  X_fillmean[is.na(c),i] = mean(c, na.rm = T)
}
for(i in catCols){
  levels(X_fillmean[,i]) = c(levels(X_fillmean[,i]), 'miss')
  X_fillmean[is.na(X_fillmean[,i]), i] = 'miss'
}
```

Fit and predict on each filled dataset using the same train/test split as with the SAEM. Here the prediction is made using any method available for caret (the choice is made at the top of the document. Here, we use a random forest estimator with no parameter tuning.)
```{r}
predictions_mice = multiple_prediction(X_imputed_mice, y, pred_method = prediction_method, 
                                  train_size = train_size, seed=seed, spl=predictions_xgb$spl)
predictions_amelia = multiple_prediction(X_imputed_amelia, y, pred_method = prediction_method, 
                                  train_size = train_size, seed=seed, spl=prediction_SAEM$spl)
predictions_mi = multiple_prediction(X_imputed_mi, y, pred_method = prediction_method, 
                                  train_size = train_size, seed=seed, spl=prediction_SAEM$spl)
#predictions_cat = multiple_prediction(X_imputed_cat, y, pred_method = prediction_method, 
#                                  train_size = train_size, seed=seed, spl=prediction_SAEM$spl)
```

Perform a single prediction on the filled dataset with the mean
```{r}
prediction_meanfill = multiple_prediction(list(X_fillmean), y, pred_method = prediction_method, 
                                          train_size = train_size, seed=seed,spl=predictions_xgb$spl)
```

Get pooled predicted values
```{r}
y_true = predictions_mice$y_true
y_pred_mice = pool_MI_binary(predictions_mice)
y_pred_mi = pool_MI_binary(predictions_mi)
y_pred_amelia = pool_MI_binary(predictions_amelia)
#y_pred_cat = pool_MI_binary(predictions_cat)

#y_pred_SAEM = prediction_SAEM$y_pred
y_pred_xgb= predictions_xgb$y_pred
y_pred_meanfill = pool_MI_binary(prediction_meanfill)
```


## Result aggregation and comparison
Compute the average predictor for multiple imputation and regroup all the predictions in a table
```{r}
results = data.frame(row.names=1:length(y_true))

results$Mice = y_pred_mice
results$mi = y_pred_mi
results$amelia = y_pred_amelia

results$meanImp = y_pred_meanfill
#results$SAEM = y_pred_SAEM
results$xgb = y_pred_xgb

results$Hemo.shock = as.numeric(y_true == 'X1')

print('Estimations of the probability of hemorragic shock by various methods')
head(results)
```

Plot the ROC curve for these estimators. The number in the legend is the AUC (are under the ROC curve) for each model.
```{r, warning=FALSE}
roc.plot(results$Hemo.shock, results[,1:5], legend=T, leg.text=c("Mice+RF", 'mi+RF', 'amelia+RF', 'Mean+RF', 'xgb'))
```

In this case, the results are extremely similar. It is interesting to note that when we restrict the dataset to 1000 entries, the SAEM logistic regression performs significantly better than the other methods. In our work to come, we will try to see how this performance discrepancy evolves when we increase the amount of missing data, as well as the performance of other imputation methods. 

Below is the importance of the variables used by the classifier (for the mean-imputed dataset)
```{r}
importances = varImp(prediction_meanfill$trained_predictors[[1]])
print(importances)

imp = as.data.frame(importances$importance)
imp$features = rownames(imp)
dput(imp %>% arrange(desc(Overall)) %>% dplyr::select(features))
```

